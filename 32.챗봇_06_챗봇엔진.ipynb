{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfca4dd7-4918-48e9-aba5-a3ea08f82659",
   "metadata": {},
   "source": [
    "# 6. 쳇봇엔진만들기\n",
    "\n",
    "## 6.1 쳇봇엔진구조\n",
    "\n",
    "<h6 align=\"center\">쳇봇 엔진의 핵심 기능</h6>\n",
    "\n",
    "|핵심기능|설명|\n",
    "|:------:|:----------------|\n",
    "|질문의도분류|화자의 질문의도를 파악, 해당 질문을 의도분류모델을 이용해 의도클래스를 예측하는 문제|\n",
    "|개체명 인식|화자의 질문에서 단어 토큰별 개체명을 인식. 이는 단어 토큰에 맞는 개체명을 예측하는 문제|\n",
    "|핵심 키워드 추출|화자질문에서 핵심단어토큰을 추출. 형태소분석기로 핵심 키워드가 되는 명사,동사를 추출|\n",
    "|답변 검색|해당질문의도, 개체명, 핵심키워드등을 기반으로 답변을 학습DB에서 검색|\n",
    "|소켓 서버|다향한종류(카카오톡, 네이버톡톡)의 챗봇 클라이언트에서 요청 질문을 처리하기 위해 소켓서버|\n",
    "||프로그램 역할을 한다. 따라서 이 책에서는 챗봇엔진 서버 프로그램이라 할 예정|\n",
    "\n",
    "## 6.2 쳇봇엔진처리과정\n",
    "\n",
    "1. 화자질의문장을 입력후 쳇봇엔진은 제일 먼저 전처리를 실행\n",
    "1. 형태소분석기를 통해 토큰을 추출후 필요한 품사(명사, 동사 등)이외의 불용어를 제거\n",
    "1. 의도분석과 개체명인식을 완료후 결과를 이용해서 적절한 답변을 학습된 DB에서 검색해서 화자에 답변을 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f48b7e-9b06-498a-877f-ff2f53645c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Preprocess.py\n",
    "# Preprocess.py(1) - 전처리로직만 작성\n",
    "from konlpy.tag import Komoran\n",
    "\n",
    "class Preprocess:\n",
    "    # 1. 생성자\n",
    "    def __init__(self, userdic=None):\n",
    "        # 1) 형태소분석기객체생성 및 초기화\n",
    "        self.kormorn = Komoran(userdic=userdic)\n",
    "        \n",
    "        # 2) 불용어제거\n",
    "        # 제외할 품사를 exclusion_tags 리스트에 정의\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언, 기호, 어미, 접미사를 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC', 'SF', 'SP', 'SS', 'SE', 'SO', 'EP', \n",
    "            'EF', 'EC', 'ETN', 'ETM', 'XSN', 'XSV', 'XSA']  \n",
    "        \n",
    "\n",
    "    # 2. 형태소분속기 pos 태거\n",
    "    def pos(self, sentence):\n",
    "        return self.kormorn.pos(sentence)\n",
    "        \n",
    "    # 3. 불용어제거후 필요한 품사정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c7e34d-3211-47b5-85f0-f9bfcdcd37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리과정 테스트\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "sentence = '내일 오전 10시에 탕수육을 주문하고 싶어'\n",
    "\n",
    "# 1. 전처리객체생성\n",
    "p = Preprocess(userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "# 2. 형태소분석\n",
    "pos = p.pos(sentence)\n",
    "print(pos)\n",
    "\n",
    "# 3. 품사태그와 키워드를 출력\n",
    "ret = p.get_keywords(pos, without_tag=False)\n",
    "print(ret)\n",
    "\n",
    "# 4. 품사태그없이 키워드를 출력\n",
    "ret = p.get_keywords(pos, without_tag=True)\n",
    "print(ret)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d45743-0e79-4cec-a8a3-aff4608e00ba",
   "metadata": {},
   "source": [
    "## 6.3 단어사전구축과 시퀀스 생성\n",
    "\n",
    "* 말뭉치데이터(corpus.txt) -> train_toos/dict폴더에 저장\n",
    "* 내일 -> 999, 오전 -> 111의 형태로 시퀀스생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f628d7-28c4-4592-9eb7-241de1ebd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\train_tools\\dict\\create_dict.py\n",
    "# create_dict.py(1) - 단어사전생성로직만 생성\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from tensorflow.keras import preprocessing\n",
    "import pickle\n",
    "\n",
    "# 1. 말뭉치데이터로딩함수 -> 말뭉치데이터를 list로 변환함수\n",
    "def read_corpus_data(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[1:]  # 헤더를 제거\n",
    "    return data\n",
    "    \n",
    "# 2. 말뭉치데이터로딩\n",
    "corpus_data = read_corpus_data('./chatbot/train_tools/dict/corpus.txt')\n",
    "\n",
    "# 3. 말뭉치데이터에서 키워드만 추출 -> 사용자사전리스트를 생성\n",
    "# corpus_data리스트에서 POS태깅후 단어리스트(dict)에 저장\n",
    "p = Preprocess()\n",
    "d = []\n",
    "for c in corpus_data:\n",
    "    pos = p.pos(c[1])\n",
    "    for k in pos:\n",
    "        d.append(k[0])\n",
    "\n",
    "# 4. 사전에 사용될 index를 생성 -> 토크나이징처리를 해서 단어리스를 단어인덱스dict데이터를 생성\n",
    "tokenizer = preprocessing.text.Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(d)\n",
    "word_index = tokenizer.word_index\n",
    "# len(word_index)\n",
    "\n",
    "# 5. 사전파일생성\n",
    "# 생성된 단어인덱스dict(word_index)객체를 파일로 저장\n",
    "f = open('./chatbot/train_tools/dict/chatbot_dict.bin', 'wb')\n",
    "try:\n",
    "    pickle.dump(word_index, f)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0cd393-5a75-48ad-8307-406fc2c323d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\train_tools\\dict\\create_dict_test.py\n",
    "# 단어사전테스트\n",
    "import pickle\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 단어사전로딩\n",
    "f = open('./chatbot/train_tools/dict/chatbot_dict.bin', 'rb')\n",
    "word_index = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "# 2. 전처리객체생성\n",
    "sentence = '내일 오전 10시에 탕수육을 주문하고 싶어 ㅋㅋ'\n",
    "p = Preprocess(userdic='./chatbot/utils/user_dic.tsv')\n",
    "pos = p.pos(sentence)\n",
    "\n",
    "# 3. 테스트문장을 입력값으로 전달받아서 키워드와 인덱스를 출력\n",
    "keywords = p.get_keywords(pos, without_tag=True)\n",
    "for word in keywords:\n",
    "    try:\n",
    "        print(word, word_index[word])\n",
    "    except KeyError:\n",
    "        # 해당단어가 사전에 없을 때 OOV로 처리\n",
    "        print(word, word_index['OOV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76d19c3-4a55-46bf-941b-3915a2b58d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install JPype1\n",
    "!pip show JPype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07b12ab-2dd1-4843-b248-0a5444f1bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Preprocess.py\n",
    "# Preprocess.py(2) - 시퀀스생성로직 작성\n",
    "# 단어인덱스 시퀀스변환 메서드를 추가\n",
    "from konlpy.tag import Komoran\n",
    "import pickle\n",
    "import jpype  # JPype는 Python 으로 하여금 거의 모든 Java 라이브러리를 사용하게 한다.\n",
    "\n",
    "class Preprocess:\n",
    "    # 1. 생성자\n",
    "    def __init__(self, word2index_dic='', userdic=None):\n",
    "        # 0) 단어인덱스사전 로딩\n",
    "        if(word2index_dic != ''):\n",
    "            f = open(word2index_dic, 'rb')\n",
    "            self.word_index = pickle.load(f)\n",
    "            f.close()\n",
    "        else:\n",
    "            self.word_index = None\n",
    "            \n",
    "         # 1) 형태소분석기객체생성 및 초기화\n",
    "        self.kormorn = Komoran(userdic=userdic)\n",
    "        \n",
    "        # 2) 불용어제거\n",
    "        # 제외할 품사를 exclusion_tags 리스트에 정의\n",
    "        # 참조 : https://docs.komoran.kr/firststep/postypes.html\n",
    "        # 관계언, 기호, 어미, 접미사를 제거\n",
    "        self.exclusion_tags = [\n",
    "            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ',\n",
    "            'JX', 'JC', 'SF', 'SP', 'SS', 'SE', 'SO', 'EP', \n",
    "            'EF', 'EC', 'ETN', 'ETM', 'XSN', 'XSV', 'XSA']  \n",
    "        \n",
    "    # 2. 형태소분속기 pos 태거\n",
    "    def pos(self, sentence):\n",
    "        jpype.attachThreadToJVM() \n",
    "        return self.kormorn.pos(sentence)\n",
    "        \n",
    "    # 3. 불용어제거후 필요한 품사정보만 가져오기\n",
    "    def get_keywords(self, pos, without_tag=False):\n",
    "        f = lambda x: x in self.exclusion_tags\n",
    "        word_list = []\n",
    "        for p in pos:\n",
    "            if f(p[1]) is False:\n",
    "                word_list.append(p if without_tag is False else p[0])\n",
    "        return word_list     \n",
    "\n",
    "    # 4. 키워드를 단어인덱스 시퀀스로 변환\n",
    "    def get_wordidx_sequence(self, keywords):\n",
    "        if self.word_index is None:\n",
    "            return []\n",
    "            \n",
    "        w2i = []\n",
    "        for word in keywords:\n",
    "            try:\n",
    "                w2i.append(self.word_index[word])\n",
    "            except KeyError:\n",
    "                w2i.append(self.word_index['OOV']) # 해당 단어가 사전에 없을 떄 OOV처리\n",
    "                \n",
    "        return w2i       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4305a8-a615-4f05-b802-9fd1634edf31",
   "metadata": {},
   "source": [
    "## 6.4 의도분류모델\n",
    "\n",
    "* 쳇봇엔진에 화자의 질의가 입력되었을 때 전처리과정을 거친후 `화자의 문장의 의도를 분류`해야 한다.\n",
    "* 클래스별로 분류하기위해서 `CNN모델을 사용`\n",
    "* 실습하는 말뭉치의 의도는 5가지분류\n",
    "\n",
    "<h6 align=\"center\">쳇봇 엔진의 의도 분류 클래스 종류</h6>\n",
    "\n",
    "|의도명|분류클래스|설명|\n",
    "|:------:|:---:|:----------------|\n",
    "|인사|0|텍스트가 인사말인 경우|\n",
    "|욕설|1|텍스트가 욕설인 경우|\n",
    "|주문|2|텍스트가 주문 관련 내용인 경우|\n",
    "|예약|3|텍스트가 예약 관련 내용인 경우|\n",
    "|기타|4|어떤 의도에도 포함되지 않는 경우|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd099fa0-d0fe-4fb2-afba-361c918ec113",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\config\\GlobalParams.py\n",
    "# 글로벌 파라미터 정보 정의\\\n",
    "# 단어시퀀스의 벡터크기\n",
    "MAX_SEQ_LEN = 15\n",
    "def GlobalParams():\n",
    "    global MAX_SEQ_LEN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a541bda8-464b-4058-8064-e0d2c45513b8",
   "metadata": {},
   "source": [
    "### 6.4.1 의도분류모델학습\n",
    "* 학습데이터 : ./chatbot/models/intent/total_train_data.csv\n",
    "  - 음식주문과 예약을 위한 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa216c2-5b78-4ebe-b157-a3b8f0154396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile .\\chatbot\\models\\intent\\train_model.py\n",
    "# 필요한 모듈 임포트\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Conv1D, GlobalMaxPool1D, concatenate\n",
    "\n",
    "\n",
    "# 데이터 읽어오기\n",
    "train_file = './chatbot/models/intent/total_train_data.csv'\n",
    "data = pd.read_csv(train_file, delimiter=',')\n",
    "queries = data['query'].tolist()\n",
    "intents = data['intent'].tolist()\n",
    "\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "p = Preprocess(word2index_dic=\"./chatbot/train_tools/dict/chatbot_dict.bin\",\n",
    "               userdic=\"./chatbot/utils/user_dic.tsv\")\n",
    "\n",
    "# 단어 시퀀스 생성\n",
    "sequences = []\n",
    "for sentence in queries:\n",
    "    pos = p.pos(sentence)\n",
    "    keywords = p.get_keywords(pos, without_tag=True)\n",
    "    seq = p.get_wordidx_sequence(keywords)\n",
    "    sequences.append(seq)\n",
    "\n",
    "\n",
    "# 단어 인덱스 시퀀스 벡터 ○2\n",
    "# 단어 시퀀스 벡터 크기\n",
    "from chatbot.config.GlobalParams import MAX_SEQ_LEN\n",
    "padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "# (105658, 15)\n",
    "print(padded_seqs.shape)\n",
    "print(len(intents)) #105658\n",
    "\n",
    "# 학습용, 검증용, 테스트용 데이터셋 생성 ○3\n",
    "# 학습셋:검증셋:테스트셋 = 7:2:1\n",
    "ds = tf.data.Dataset.from_tensor_slices((padded_seqs, intents))\n",
    "ds = ds.shuffle(len(queries))\n",
    "\n",
    "train_size = int(len(padded_seqs) * 0.7)\n",
    "val_size = int(len(padded_seqs) * 0.2)\n",
    "test_size = int(len(padded_seqs) * 0.1)\n",
    "\n",
    "train_ds = ds.take(train_size).batch(20)\n",
    "val_ds = ds.skip(train_size).take(val_size).batch(20)\n",
    "test_ds = ds.skip(train_size + val_size).take(test_size).batch(20)\n",
    "\n",
    "# 하이퍼 파라미터 설정\n",
    "dropout_prob = 0.5\n",
    "EMB_SIZE = 128\n",
    "EPOCH = 5\n",
    "VOCAB_SIZE = len(p.word_index) + 1 #전체 단어 개수\n",
    "\n",
    "\n",
    "# CNN 모델 정의  ○4\n",
    "input_layer = Input(shape=(MAX_SEQ_LEN,))\n",
    "embedding_layer = Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN)(input_layer)\n",
    "dropout_emb = Dropout(rate=dropout_prob)(embedding_layer)\n",
    "\n",
    "conv1 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=3,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool1 = GlobalMaxPool1D()(conv1)\n",
    "\n",
    "conv2 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=4,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool2 = GlobalMaxPool1D()(conv2)\n",
    "\n",
    "conv3 = Conv1D(\n",
    "    filters=128,\n",
    "    kernel_size=5,\n",
    "    padding='valid',\n",
    "    activation=tf.nn.relu)(dropout_emb)\n",
    "pool3 = GlobalMaxPool1D()(conv3)\n",
    "\n",
    "# 3,4,5gram 이후 합치기\n",
    "concat = concatenate([pool1, pool2, pool3])\n",
    "\n",
    "hidden = Dense(128, activation=tf.nn.relu)(concat)\n",
    "dropout_hidden = Dropout(rate=dropout_prob)(hidden)\n",
    "logits = Dense(5, name='logits')(dropout_hidden)\n",
    "predictions = Dense(5, activation='softmax')(logits)\n",
    "\n",
    "\n",
    "# 모델 생성  ○5\n",
    "model = Model(inputs=input_layer, outputs=predictions)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# 모델 학습 ○6\n",
    "model.fit(train_ds, validation_data=val_ds, epochs=EPOCH, verbose=1)\n",
    "\n",
    "# 모델 평가(테스트 데이터 셋 이용) ○7\n",
    "loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy * 100))\n",
    "print('loss: %f' % (loss))\n",
    "\n",
    "# 모델 저장  ○8\n",
    "model.save('./chatbot/models/intent/intent_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea25eabf-8208-40a2-8c18-0c7bb9c4be16",
   "metadata": {},
   "source": [
    "### 6.4.2 의도분류모델생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918ca4b-90a8-4462-aeb8-bc53e0dd6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\IntentModel.py\n",
    "# 쳇봇엔진 - 의도분류모델로딩(모델재사용)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 의도분류모델모듈\n",
    "class IntentModel:\n",
    "\n",
    "    # 1) 생성자\n",
    "    def __init__(self, model_name, preprocess):\n",
    "        # 의도클래스레이블\n",
    "        self.labels = {0:'인사', 1:'욕설', 2:'주문', 3:'예약', 4:'기타'}\n",
    "\n",
    "        # 훈련된 의도분류모델 로딩\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # chatbot.Preprocess\n",
    "        self.p = preprocess\n",
    "\n",
    "    # 2) 의도클래스 예측함수\n",
    "    def predict_class(self, query):\n",
    "        # 1) 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "\n",
    "        # 2) 문장(query)내에서 키워드추출, 불용어제거\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 3) 벡터의 최대크기\n",
    "        from chatbot.config.GlobalParams import MAX_SEQ_LEN\n",
    "        \n",
    "        # 4) 패딩처리\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "\n",
    "        # 5) 예측\n",
    "        predict = self.model.predict(padded_seqs)\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "\n",
    "        # 6) 예측결과 즉, 의도클래스를 반환\n",
    "        return predict_class.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6072eae5-45b6-4b07-8a86-773e8190e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\intent\\model_intent_test.py\n",
    "# chatbot 엔진 - 의도분류모델을 테스트\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "\n",
    "p = Preprocess(word2index_dic='./chatbot/train_tools/dict/chatbot_dict.bin'\n",
    "               , userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "intent = IntentModel(model_name='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "\n",
    "query = '오늘 탕수육 주문 가능한가요?'\n",
    "predict = intent.predict_class(query=query)\n",
    "predict_label = intent.labels[predict]\n",
    "\n",
    "print(query)\n",
    "print(f'발화자의 질의를 예측한 의도의 클래스 = {predict}')\n",
    "print(f'발화자의 질의를 예측한 의도의 레이블 = {predict_label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba413cfc-c583-4a20-8750-052d8ac421e6",
   "metadata": {},
   "source": [
    "## 6.5 개체명인식모델학습\n",
    "\n",
    "* 개체명인식모델을 `양방향LSTM모델을 사용`\n",
    "\n",
    "<h6 align=\"center\">개체명종류</h6>\n",
    "\n",
    "|개체명|설명|\n",
    "|:----:|:-----------------|\n",
    "|B_FOOD|음식|\n",
    "|B_DT, B_TI|날짜,시간(학습데이터의 영향으로 날짜와 시간을 혼용해서 사용)|\n",
    "|B_PS|사람|\n",
    "|B_OG|조직, 회사|\n",
    "|B_LC|지역|\n",
    "\n",
    "### 6.5.1 개체명인식모델 데이터셋\n",
    "\n",
    "* 학습용데이터셋 : ./chatbot/models/ner/ner_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f2003-2742-4297-bad8-7148c0127285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile .\\chatbot\\models\\ner\\train_model.py\n",
    "# 챗봇엔진 - NER모델생성\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 학습파일로딩\n",
    "def read_file(file_name):\n",
    "    sents = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, l in enumerate(lines):\n",
    "            if l[0] == ';' and lines[idx+1][0] == '$':\n",
    "                this_sent = []\n",
    "            elif l[0] == '$' and lines[idx-1][0] == ';':\n",
    "                continue\n",
    "            elif l[0] == '\\n':\n",
    "                sents.append(this_sent)\n",
    "            else:\n",
    "                this_sent.append(tuple(l.split()))\n",
    "\n",
    "    return sents\n",
    "\n",
    "p = Preprocess(word2index_dic='./chatbot/train_tools/dict/chatbot_dict.bin'\n",
    "               , userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "# 2. 학습용말뭉치데이터 로딩\n",
    "corpus = read_file('./chatbot/models/ner/ner_train.txt')\n",
    "\n",
    "# 3. 말뭉치데이터에서 단어(2번째), BIO태그(4번째)만 로딩해서 학습용데이터셋을 생성\n",
    "sentences, tags = [], []\n",
    "for t in corpus:\n",
    "    tagged_sentence = []\n",
    "    sentence, bio_tag = [], []\n",
    "    for w in t:\n",
    "        tagged_sentence.append((w[1], w[3]))\n",
    "        sentence.append(w[1])\n",
    "        bio_tag.append(w[3])\n",
    "\n",
    "    sentences.append(sentence)\n",
    "    tags.append(bio_tag)\n",
    "\n",
    "print(f'샘플데이터셋의 크기 = \\n {len(sentences)}')\n",
    "print(f'0번째 샘플단어의 시퀀스 = \\n {sentences[0]}')\n",
    "print(f'0번째 샘플단어의 BIO태그 = \\n {tags[0]}')\n",
    "print(f'샘플단어의 시퀀스의 최대길이 = \\n {max(len(l) for l in sentences)}')\n",
    "print(f'샘플단어의 시퀀스의 평균길이 = \\n {sum(map(len, sentences))/len(sentences)}')\n",
    "\n",
    "# 4. 토크나이저 정의\n",
    "# 단어시퀀스는 Preprocess객체에서 생성하기 때문에 BIO태그용 토크나이저 객체만 생성\n",
    "tag_tokenizer = preprocessing.text.Tokenizer(lower=False) # 태그정보는 소문자로 변환하지 않는다.\n",
    "tag_tokenizer.fit_on_texts(tags)\n",
    "\n",
    "# 단어사전 및 태그사전의 크기\n",
    "vocab_size = len(p.word_index) + 1\n",
    "tag_size = len(tag_tokenizer.word_index) + 1\n",
    "print(f'BIO태그사전의 크기 = {tag_size}')\n",
    "print(f'단어사전의 크기 = {vocab_size}')\n",
    "\n",
    "# 5. 학습용 단어시퀀스 생성\n",
    "# BIO태그는 토크나이저에서 생성된 사전데이터를 시퀀스번호형태로 인코딩한다.\n",
    "X_train = [p.get_wordidx_sequence(sent) for sent in sentences]\n",
    "y_train = tag_tokenizer.texts_to_sequences(tags)\n",
    "\n",
    "index_to_ner = tag_tokenizer.index_word # 시퀀스인덱스를 NER로 변환하기위해 사용\n",
    "index_to_ner[0] = 'PAD'\n",
    "\n",
    "# 6. 시퀀스패딩처리\n",
    "# 개체명인식모델의 입출력크기를 동일하게 설정하기 위해 시퀀스 패딩처리를 실행\n",
    "# 벡터크기를 단어 시퀀스의 평균길이보다 여유있게 40으로 설정\n",
    "max_len = 40\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, padding='post', maxlen=max_len)\n",
    "y_train = preprocessing.sequence.pad_sequences(y_train, padding='post', maxlen=max_len)\n",
    "\n",
    "# 7. 학습용 vs 검증용 = 8:2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train\n",
    "                                                    , test_size=.2, random_state=1234) \n",
    "X_train.shape, X_test.shape\n",
    "\n",
    "# 8. 출력된 데이터를 one-hot encoding 처리\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=tag_size)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=tag_size)\n",
    "print(f'학습용 샘플데이터셋 시퀀스 크기 = {X_train.shape}')\n",
    "print(f'학습용 샘플데이터셋 레이블 크기 = {y_train.shape}')\n",
    "print(f'검증용 샘플데이터셋 시퀀스 크기 = {X_test.shape}')\n",
    "print(f'검증용 샘플데이터셋 레이블 크기 = {y_test.shape}')\n",
    "\n",
    "# 9. 모델정의(Bi-LSTM)\n",
    "# tag_size만큼 출력 뉴런에서 제일 확률은 출력값 1개를 선택하기 위해 softmax()함수 사용\n",
    "# 손실함수는 categorical_crossentropy를 사용\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=30, input_length=max_len, mask_zero=True))\n",
    "model.add(Bidirectional(LSTM(200, return_sequences=True, dropout=0.5, recurrent_dropout=0.25)))\n",
    "model.add(TimeDistributed(Dense(tag_size, activation='softmax')))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(0.01), metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10)\n",
    "\n",
    "print(f'평가결과(정확도) = {model.evaluate(X_test, y_test)[1]}')\n",
    "\n",
    "model.save('./chatbot/models/ner/ner_model.keras')\n",
    "\n",
    "# 10. 시퀀스를 NER태그로 변환\n",
    "# 예측값을 index_to_ner함수를 이용해서 태깅정보를 변환하는 함수 작성\n",
    "def sequences_to_tag(sequences):\n",
    "    result = []\n",
    "    for sequence in sequences: # 전체시퀀스(sequences)애서 시퀀스를 하나씩 꺼내오기\n",
    "        temp = []\n",
    "        for pred in sequence:  # 시퀀스로 부터 예측값을 하나씩 꺼내오기\n",
    "            pred_index = np.argmax(pred)\n",
    "            temp.append(index_to_ner[pred_index].replace('PAD', 'O')) # 패딩처리된 타입 PAD를 기타(O)로 변경\n",
    "        result.append(temp)\n",
    "\n",
    "    return result\n",
    "\n",
    "# 11. 테스트데이터셋의 NER예측 \n",
    "# 1) fi_score를 계산하기 위해 import\n",
    "#    predict()함수를 이용해서 f1-score값을 리턴\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "# 2) 테스트데이터셋으로 예측\n",
    "#    X_test 데이터셋 시퀀스번호로 인코딩된 데이터셋(단어시퀀스, numpy배열)\n",
    "#    테스트한 후 결과가 '예측된 NER태그정보가 저장된 numpy배열'을 리턴\n",
    "y_predicted = model.predict(X_test)\n",
    "pred_tags = sequences_to_tag(y_predicted)  # 예측된 개체인식명(NER)\n",
    "test_tags = sequences_to_tag(y_test)       # 실제 개체인식명\n",
    "\n",
    "# 3) f1_score 결과\n",
    "#    classfication_report함수로 NER태그별로 계산된 정밀도, 재현율, f1_score를 출력\n",
    "print(classification_report(test_tags, pred_tags))\n",
    "print(f'f1-score = {f1_score(test_tags, pred_tags):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82dd01e-bddb-4a0d-8b34-f497f1f1e5bc",
   "metadata": {},
   "source": [
    "### 6.5.2 개체명 인식 모듈 작성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b350fc9-1c4c-4131-b10a-1e35883cd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\ner\\NerModel.py\n",
    "# 쳇봇엔진의 NER모델\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "# 개체명인식모델 모듈\n",
    "class NerModel:\n",
    "\n",
    "    # 1) 생성자\n",
    "    def __init__(self, model_name, preprocess):\n",
    "        \n",
    "        # BIO태그클래스별 레이블정의\n",
    "        self.index_to_ner = {1: 'O', 2: 'B_DT', 3: 'B_FOOD', 4: 'I', 5: 'B_OG', \n",
    "                     6: 'B_PS', 7: 'B_LC', 8: 'NNP', 9: 'B_TI', 0: 'PAD'} \n",
    "        # 의도분류모델을 로딩\n",
    "        self.model = load_model(model_name)\n",
    "\n",
    "        # 챗봇 Preprocess객체\n",
    "        self.p = preprocess\n",
    "    \n",
    "    # 2) 개체명클래스예측메서드\n",
    "    def predict(self, query):\n",
    "        # 1) 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 2) 문장내에서 키워드와 불용어제거\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "        \n",
    "        # 3) 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding='post'\n",
    "                                                           , value=0, maxlen=max_len)\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "\n",
    "        tags = [self.index_to_ner[i] for i in predict_class.numpy()[0]]\n",
    "\n",
    "        return list(zip(keywords, tags))\n",
    "\n",
    "    # 3) 예측된 개체명클래스를 태깅메서드\n",
    "    def predict_tags(self, query):\n",
    "        # 1) 형태소분석\n",
    "        pos = self.p.pos(query)\n",
    "        \n",
    "        # 2) 문장내에서 키워드와 불용어제거\n",
    "        keywords = self.p.get_keywords(pos, without_tag=True)\n",
    "        sequences = [self.p.get_wordidx_sequence(keywords)]\n",
    "\n",
    "        # 3) 패딩처리\n",
    "        max_len = 40\n",
    "        padded_seqs = preprocessing.sequence.pad_sequences(sequences, padding='post'\n",
    "                                                           , value=0, maxlen=max_len)\n",
    "        predict = self.model.predict(np.array([padded_seqs[0]]))\n",
    "        predict_class = tf.math.argmax(predict, axis=1)\n",
    "\n",
    "        tags = []\n",
    "        for tag_idx in predict_class.numpy()[0]:\n",
    "            if tag_idx == 1: continue\n",
    "            tags.append(self.index_to_ner[tag_idx])\n",
    "\n",
    "        if len(tags) == 0: return None\n",
    "        return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c9aad-0416-40b1-91a3-29f6671718dc",
   "metadata": {},
   "source": [
    "##### NerModel.py 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a87f6f-dafa-4089-b293-9bfcbc4240e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\models\\ner\\model_intent_test.py\n",
    "# NerModel 모듈 사용(1)\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.ner.NerModel import NerModel\n",
    "\n",
    "p = Preprocess(word2index_dic='./chatbot/train_tools/dict/chatbot_dict.bin'\n",
    "               , userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "ner = NerModel(model_name='./chatbot/models/ner/ner_model.keras', preprocess=p)\n",
    "query = '오늘 오전 13시 10분에 탕수육을 주문하고 싶어요'\n",
    "predicts = ner.predict(query)\n",
    "print(predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3cedb7-ac56-4fb9-81e8-a7cd1481238f",
   "metadata": {},
   "source": [
    "## 6.6 답변검색\n",
    "\n",
    "* 발화자로 부터 입력된 문장을 전처리, 의도분류, 개체명인식과정을 통해 적절한 답변을 학습DB에서 검색\n",
    "* 챗봇엔진이 자연어처리를 통해서 해석한 문장을 기초로 유사한 답변을 검색(DB에서 검색)\n",
    "* 실제 상용화된 챗봇은 여건상 구현하기 힘들기 때문에 단순한 검색수준의 SQL을 이용한 DB기반으로 답변을 검색하는 방법을 구현\n",
    "\n",
    "### 6.6.1 DB제어모듈생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeab7c-41c7-463b-bae5-7a76ba1b081f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile .\\chatbot\\utils\\Database.py\n",
    "# DB제어모듈\n",
    "import pymysql\n",
    "import pymysql.cursors\n",
    "import logging\n",
    "\n",
    "class Database:\n",
    "    '''\n",
    "        Chatbot의 Database 제어 모듈...\n",
    "    '''\n",
    "    def __init__(self, host, user, password, db_name, charset='utf8'):\n",
    "        self.host = host\n",
    "        self.user = user\n",
    "        self.password = password\n",
    "        self.db_name = db_name\n",
    "        self.charset = charset\n",
    "        self.conn = None\n",
    "        \n",
    "    def connect(self):\n",
    "        if self.conn != None: return\n",
    "\n",
    "        self.conn = pymysql.connect(\n",
    "            host = self.host,\n",
    "            user = self.user,\n",
    "            password = self.password,\n",
    "            db = self.db_name,\n",
    "            charset = self.charset   \n",
    "        )\n",
    "        \n",
    "    def close(self):\n",
    "        if self.conn is None: return\n",
    "\n",
    "        if not self.conn.open:\n",
    "            self.conn = None\n",
    "            return\n",
    "\n",
    "        self.conn.close()\n",
    "        self.conn = None\n",
    "\n",
    "    # insert, delete, update\n",
    "    def execute(self, sql):\n",
    "        last_row_id = -1\n",
    "        try:\n",
    "            with self.conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "            self.conn.commit()\n",
    "            last_row_id = cursor.lastrowid\n",
    "            logging.debug(\"execute last_row_is : %d\", last_row_id) \n",
    "        except Exception as e:\n",
    "            logging.error(e)    \n",
    "\n",
    "    def select_one(self, sql):\n",
    "        result = None\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchone()\n",
    "        except Exception as e:\n",
    "           logging.error(e)\n",
    "        finally:\n",
    "            return result\n",
    "\n",
    "    def select_all(self, sql):\n",
    "        result = None\n",
    "        try:\n",
    "            with self.conn.cursor(pymysql.cursors.DictCursor) as cursor:\n",
    "                cursor.execute(sql)\n",
    "                result = cursor.fetchall()\n",
    "        except Exception as e:\n",
    "           logging.error(e)\n",
    "        finally:\n",
    "            return result        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbcfee9-0915-45fa-a988-a21639175fe9",
   "metadata": {},
   "source": [
    "### 6.6.2 답변검색모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5602fee9-c767-4fc0-a5a4-0ec8d115b350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./chatbot/utils/FindAnswer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./chatbot/utils/FindAnswer.py\n",
    "# 답변검색모듈\n",
    "class FindAnswer:\n",
    "\n",
    "    def __init__(self, db):\n",
    "        self.db = db\n",
    "\n",
    "    # 1. 검색SQL문장작성\n",
    "    def make_query(self, intent_name, ner_tags):\n",
    "        sql = 'select * from chatbot_train_data'\n",
    "        if intent_name != None and ner_tags == None:\n",
    "            sql = sql + f\" where intent = '{intent_name}'\"\n",
    "        elif intent_name != None and ner_tags != None:\n",
    "            where = f\" where intent = '{intent_name}'\"\n",
    "            if(len(ner_tags)>0):\n",
    "                where += \" and (\"\n",
    "                for ne in ner_tags:\n",
    "                    where += f\" ner like '%{ne}%' or\"\n",
    "                where = where[:-3] + \")\"\n",
    "            sql = sql + where\n",
    "\n",
    "        # 동일한 답변이 2개이상인 경우, 랜덤으로 선택\n",
    "        sql = sql + \" order by rand() limit 1\"\n",
    "\n",
    "        return sql\n",
    "\n",
    "    # 2. 답변검색\n",
    "    # 의도명(intent_name)과 태그리스트(ner_tags)를 이용해서 질문의 답변을 검새\n",
    "    def search(self, intent_name, ner_tags):\n",
    "\n",
    "        # 1) 의도명, 개체인식명으로 답변검색\n",
    "        sql = self.make_query(intent_name, ner_tags)\n",
    "        answer = self.db.select_one(sql)\n",
    "        \n",
    "        # 2) 검색되는 답변이 없을 경우 의도명만 검색\n",
    "        if answer is None:\n",
    "            sql = self.make_query(intent_name, None)\n",
    "            answer = self.db.select_one(sql)\n",
    "\n",
    "        return (answer['answer'], answer['answer_image'])\n",
    "\n",
    "    # 3. NER태그를 실제로 입력된 단어로 변환하는 함수\n",
    "    # 질문 : 탕수육 대자로 한개 주문할게요 -> 개체명인식명 탕수육 B_FOOD로 처리\n",
    "    # 답변 : {B_FOOD} 주문할게요\t{B_FOOD} 주문 처리 완료되었습니다. \n",
    "    def tag_to_word(self, ner_predicts, answer):\n",
    "        for word, tag in ner_predicts:\n",
    "            # 변환해야하는 태그가 있는 경우 추가\n",
    "            if tag=='B_FOOD' or tag=='B_DT' or tag=='B_TI':\n",
    "                answer = answer.replace(tag, word) # {B_FOOD} -> {탕수육}\n",
    "                \n",
    "        answer = answer.replace('{', '')\n",
    "        answer = answer.replace('}', '')\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2fe5a-1c53-478d-942b-bfa43e77781e",
   "metadata": {},
   "source": [
    "### 6.6.3 챗봇엔진 동작 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74b30d11-3042-454c-9eac-9e0188cdbf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./chatbot/test/chabot_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./chatbot/test/chabot_test.py\n",
    "# chatbot엔진 동작 테스트하기\n",
    "from chatbot.config.DatabaseConfig import *\n",
    "from chatbot.utils.Database import Database\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "\n",
    "# 1. 전처리객체생성\n",
    "p = Preprocess(word2index_dic='./chatbot/train_tools/dict/chatbot_dict.bin'\n",
    "               , userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "# 2. DB객체생성\n",
    "db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "db.connect()\n",
    "\n",
    "# 3. 발화자질의\n",
    "query = '오전에 탕수육 10개를 주문합니다'\n",
    "# query = '오전에 탕수육  주문합니다'\n",
    "# query = '화자의 질문의도를 파악합니다.'\n",
    "# query = '안녕하세요'\n",
    "# query = '자장면 주문할게요'\n",
    "\n",
    "# 4. 발화자의도파악\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "intent = IntentModel(model_name='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "predict = intent.predict_class(query)\n",
    "intent_name = intent.labels[predict]\n",
    "\n",
    "# 5. 개체명인식\n",
    "from chatbot.models.ner.NerModel import NerModel\n",
    "ner = NerModel(model_name='./chatbot/models/ner/ner_model.keras', preprocess=p)\n",
    "predicts = ner.predict(query)\n",
    "ner_tags = ner.predict_tags(query)\n",
    "\n",
    "# 6. 출력확인\n",
    "print(f'발화자의 질의 = {predict}')\n",
    "print(f'발화자의 의도 = {intent_name}')\n",
    "print(f'발화자의 질의의 개체명 = {predicts}')\n",
    "print(f'발화자의 질의의 NER태그(답변검색에 필요한 NER태그) = {ner_tags}')\n",
    "\n",
    "# 7. 답변검색\n",
    "from chatbot.utils.FindAnswer import FindAnswer\n",
    "\n",
    "try:\n",
    "    f = FindAnswer(db)\n",
    "    # print(f.make_query(intent_name, ner_tags))\n",
    "    answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "    answer = f.tag_to_word(predicts, answer_text)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    answer = \"죄송합니다. 무슨 말인지 모르겠어요!\"\n",
    "\n",
    "print(f'답변검색결과 = {answer}')\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffaec2-ec80-428b-852c-06c75b21404b",
   "metadata": {},
   "source": [
    "## 6.7 챗봇엔진서버\n",
    "\n",
    "* 챗봇API인 카카카오톡이나 네이버톡톡같은 메신저 플랫폼을 이용\n",
    "* 실습은 카카오톡 API를 사용\n",
    "\n",
    "### 6.7.1 통신프로토콜정의\n",
    "\n",
    "* 서버와 클라이언트간 JSON형태로 통신\n",
    "\n",
    "```json\n",
    "# 질의 텍스트\n",
    "{\n",
    "    \"Query\": \"자장면 주문할게요\",\n",
    "    \"BotType\": \"Kakao\"\n",
    "}\n",
    "\n",
    "# 답변\n",
    "{\n",
    "    \"Query\": \"자장면 주문할게요\",\n",
    "    \"Intent\": \"주문\",\n",
    "    \"NER\":\"[('자장면', 'B_FOOD'), ('주문', 'O)]\",\n",
    "    \"Answer\":\"자장면 주문 처리 감사\",\n",
    "    \"AnswerImageUrl\":\"\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a089f1-430e-43cf-8d08-463bad6422ed",
   "metadata": {},
   "source": [
    "### 6.7.2 챗봇서버모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c49f01f8-add9-40a9-aa5a-8697f4487357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./chatbot/utils/BotServer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./chatbot/utils/BotServer.py\n",
    "# 챗봇서버모듈\n",
    "import socket\n",
    "\n",
    "class BotServer:\n",
    "    # 1. chatbot의 서버포트번화 동시접속자수를 정의\n",
    "    def __init__(self, srv_port, listen_num):\n",
    "        self.port = srv_port # 서버접속포트번호\n",
    "        self.listen = listen_num\n",
    "        self.mySock = None\n",
    "        \n",
    "    # 2. socket생성\n",
    "    #    파이썬에서 지원하는 저수준 네트워킹인터페이스 API를 사용하기 쉽게 작성된 랩퍼함수\n",
    "    #    TCP/IP 소켓생성후 접속자수 만큼 클라이언트의 연결을 수락\n",
    "    def create_socket(self):\n",
    "        # self.mySock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        self.mySock = socket.socket()\n",
    "        self.mySock.bind((\"0.0.0.0\", int(self.port)))\n",
    "        self.mySock.listen(int(self.listen))\n",
    "        return self.mySock\n",
    "        \n",
    "    # 3. 클라이언트 대기후 연결을 수락하는 메서드\n",
    "    #    연결요청시 클라이언트와 통신가능한 소켓객체를 리턴\n",
    "    #    반환값(conn, address)을 tuple형태로 리턴\n",
    "    def ready_for_client(self):\n",
    "        return self.mySock.accept()\n",
    "        \n",
    "    # 4. socket반환\n",
    "    def get_socket(self):\n",
    "        return self.mySock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf93ede-2736-4303-8d2b-927f638788e9",
   "metadata": {},
   "source": [
    "### 6.7.3 챗봇서버 메인 프로그램\n",
    "* 실행방법 : python[.exe] bot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92315c37-fccc-4c5d-9bf9-a701de0df07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./bot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./bot.py\n",
    "import threading\n",
    "import json\n",
    "\n",
    "from chatbot.config.DatabaseConfig import *\n",
    "from chatbot.utils.Database import Database\n",
    "from chatbot.utils.BotServer import BotServer\n",
    "from chatbot.utils.Preprocess import Preprocess\n",
    "from chatbot.models.intent.IntentModel import IntentModel\n",
    "from chatbot.models.ner.NerModel import NerModel\n",
    "from chatbot.utils.FindAnswer import FindAnswer\n",
    "\n",
    "# 1. 전처리\n",
    "p = Preprocess(word2index_dic='./chatbot/train_tools/dict/chatbot_dict.bin'\n",
    "               , userdic='./chatbot/utils/user_dic.tsv')\n",
    "\n",
    "# 2. 의도파악학습모델\n",
    "intent = IntentModel(model_name='./chatbot/models/intent/intent_model.keras', preprocess=p)\n",
    "\n",
    "# 3. 개체인식학습모델\n",
    "ner = NerModel(model_name='./chatbot/models/ner/ner_model.keras', preprocess=p)\n",
    "\n",
    "# 4. 클라이언트가 연결되는 순간 실행되는 Thread함수\n",
    "#    적절한 답변을 검색한후에 요청(requext)한 클라이언트에 응답(response)\n",
    "def to_client(conn, addr, params):\n",
    "    db = params['db']\n",
    "\n",
    "    try:\n",
    "        db.connect()\n",
    "\n",
    "        # 1) data 수신(발화자의 질의)\n",
    "        # conn은 쳇봇클라이언트의 소캣객체, recv()메서드는 데이터의\n",
    "        # 수신이 완료될 때까지 블럭킹, 최대 2048bytes만큼 데이터를 수신\n",
    "        # 에러(연결중단 or 예외)가 있을 경우에 recv()메서드는 None을 리턴\n",
    "        read = conn.recv(2048)  # 수신데이터가 있을 때 까지 블럭킹\n",
    "        print('='*60)\n",
    "        print(f'Connection from : {str(addr)}') # localhost:5000?....\n",
    "\n",
    "        if read is None or not read: # 클라이언트연결중단 or 에러가 있을 겨우\n",
    "            print('Client Connection Stop!!')\n",
    "            exit(0)\n",
    "        \n",
    "        # 2) json형태로 변환\n",
    "        recv_json_data = json.loads(read.decode()) \n",
    "        print(f'데이터수신 : {recv_json_data}')\n",
    "        query = recv_json_data['Query']\n",
    "        \n",
    "        # 3) 발화자의 의도파악\n",
    "        intent_predict = intent.predict_class(query)\n",
    "        intent_name = intent.labels[intent_predict]\n",
    "    \n",
    "        # 4) 개체명 인식\n",
    "        ner_predicts = ner.predict(query)\n",
    "        ner_tags = ner.predict_tags(query)\n",
    "\n",
    "        # 5) DB에서 답변검색\n",
    "        try:\n",
    "            f = FindAnswer(db)\n",
    "            answer_text, answer_image = f.search(intent_name, ner_tags)\n",
    "            answer = f.tag_to_word(ner_predicts, answer_text)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            answer = \"죄송합니다. 무슨 말인지 모르겠어요! 조금 더 학습을 할게요 ㅠㅠ\"\n",
    "            answer_image = None\n",
    "\n",
    "        send_json_data_str = {\n",
    "            \"Query\": query,\n",
    "            \"Answer\": answer,\n",
    "            \"AnswerImageUrl\": answer_image,\n",
    "            \"Intent\": intent_name,\n",
    "            \"NER\": ner_tags   \n",
    "        }\n",
    "\n",
    "        message = json.dumps(send_json_data_str)\n",
    "        conn.send(message.encode())\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if db is not None:\n",
    "            db.close()\n",
    "\n",
    "# 5. chatbot application start\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 1) 답변검색을 위한 DB 연결\n",
    "    db = Database(host=DB_HOST, user=DB_USER, password=DB_PASSWORD, db_name=DB_NAME)\n",
    "    print('DB Connection Successful!!')\n",
    "\n",
    "    port = 5000\n",
    "    listen = 100\n",
    "\n",
    "    # 2) 쳇봇서버동작 - 챗봇클라이언트 연결을 대기(무한 loop)\n",
    "    bot = BotServer(port, listen)\n",
    "    bot.create_socket()\n",
    "    print('ChatBot Server Start!!')\n",
    "\n",
    "    while True:\n",
    "        conn, addr = bot.ready_for_client()\n",
    "        params = {\n",
    "            'db': db\n",
    "        }\n",
    "\n",
    "        client = threading.Thread(target=to_client, args=(conn, addr, params))\n",
    "        client.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6177d21b-48a6-4a25-bec3-28898c56aeda",
   "metadata": {},
   "source": [
    "### 6.7.4 챗봇 클라이언트 프로그램\n",
    "* cd ./lec/05.python\n",
    "* cmd창 : python ./chatbot/test/chatbot_client_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "912be421-f519-45c3-aab7-ebcd68201554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./chatbot/test/chatbot_client_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./chatbot/test/chatbot_client_test.py\n",
    "# chatbot 클라이언트 테스트 프로그램\n",
    "import socket\n",
    "import json\n",
    "\n",
    "# 1. 쳇봇엔진서버접속정보\n",
    "host = \"127.0.0.1\"\n",
    "port = 5000\n",
    "\n",
    "# 2. 클라이언트프로그램 Start\n",
    "while True:\n",
    "    query = input('질문을 입력하세요(작업종료는 q) => ') # 발화자의 질의\n",
    "    print(f'발화자질문 : {query}')\n",
    "    if(query=='q'): exit(0)\n",
    "\n",
    "    print('='*60)\n",
    "    mySocket = socket.socket()\n",
    "    mySocket.connect((host, port))\n",
    "\n",
    "    # 1) 챗봇엔진에 질의 요청\n",
    "    json_data = {\n",
    "        \"Query\": query,  \n",
    "        \"BotType\": \"myBotService\"\n",
    "    }\n",
    "    message = json.dumps(json_data)\n",
    "    mySocket.send(message.encode())\n",
    "\n",
    "    # 2) 쳇본엔진에 답변출력\n",
    "    data = mySocket.recv(2048).decode()\n",
    "    ret_data = json.loads(data)\n",
    "    print(f\"답변 = {ret_data['Answer']}\")\n",
    "    print(type(ret_data), ret_data)\n",
    "\n",
    "    # 3) 챗봇서버에 연결될 소켓 해제\n",
    "    mySocket.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cfa742-157e-4d80-b52b-cb95450e5e56",
   "metadata": {},
   "source": [
    "### 6.7.5 챗봇 애플리케이션 시작\n",
    "\n",
    "* cmd창(Termnal)\n",
    "  1. 챗봇서버 시작       : root폴더 >python bot.py\n",
    "  2. 챗봇클라이언트 시작 : root폴더 > python ./chatbot/test/chatbot_client_test.py\n",
    "* 주의할 점\n",
    "  - 서버가 비정상적으로 종료가 될 때 사용중인 port(5000)는 계속해서 활성화 상태일 수 있다.\n",
    "  - 이때, 활성화된 process id를 확인 후에 해당 작업을 kill을 시켜야 한다.\n",
    "  - `netstat -ano | findstr 5000`명령으로 process를 찾은 후에\n",
    "  - `taskkill /f /pid 검색된process-id` 명령을 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebfe26a-de42-4455-8d9c-89638c6cc4bf",
   "metadata": {},
   "source": [
    "## 6.8 맺음말\n",
    "\n",
    "1. 실제 음식주문용 챗봇을 사용하려면 발화자의 요청이 2개 이상의 B_FOOD를 인식해야 하고\n",
    "2. 음식주문수량을 확인할 수 있는 개체명이 추가 되어야 한다.\n",
    "3. 현재, 실습한 음식주문챗봇은 `자장면 1개, 탕수육 대2개 주문할게요`와 같은 주문은 정확하게 처리할 수 없다.\n",
    "4. 정확한 주문을 처리하는 챗봇을 만들이 위해서 학습데이터와 개체명인식데이터가 필요하다.\n",
    "5. 이와 같이 딥러닝모델에서는 학습용데이터가 매우 중요하다.\n",
    "6. 우리가 목표로 하는 시스템에 맞는 `데이터수집, 정제하는데 대부분의 시간이 필요`하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
